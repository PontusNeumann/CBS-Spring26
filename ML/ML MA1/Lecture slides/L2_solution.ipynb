{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header_cell",
            "metadata": {},
            "source": [
                "# KAN-CDSCO2004U  Machine Learning and Deep Learning\n",
                "\n",
                "## Lab 2: Data Cleaning and Pipelines with the Penguins Dataset\n",
                "**Estimated time: 2 hours**\n",
                "\n",
                "### Learning Objectives\n",
                "By the end of this exercise, you will be able to:\n",
                "- Perform Exploratory Data Analysis (EDA) including skewness and correlation checks\n",
                "- Identify and fix data inconsistencies (e.g., typos in categorical variables)\n",
                "- Handle missing values using robust imputation strategies\n",
                "- Manually encode categorical features (One-Hot vs Label Encoding)\n",
                "- **Build production-ready Preprocessing Pipelines** using `ColumnTransformer`\n",
                "- **Split data** into training and test sets"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "instruction_cell",
            "metadata": {},
            "source": [
                "In this exercise, you will practice the full data preprocessing workflow using the **Palmer Penguins** dataset.\n",
                "\n",
                "**How to work through this notebook:**\n",
                "- üèÉ **RUN** cells = Just execute the code to see the output\n",
                "- ‚úèÔ∏è **TODO** cells = Write your own code or answer questions\n",
                "- üìñ **READ** cells = Explanations to help you understand the concepts"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup_header",
            "metadata": {},
            "source": [
                "---\n",
                "## Setup\n",
                "\n",
                "üèÉ **RUN** the cells below to import libraries and load the data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import needed libraries\n",
                "# Author: Luca Gudi (lgg.digi@cbs.dk)\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# SKLearn preprocessing tools\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, OrdinalEncoder, LabelEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Display options\n",
                "pd.set_option('display.max_columns', None)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the Penguins dataset\n",
                "url = \"https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\"\n",
                "df_raw = pd.read_csv(url)\n",
                "\n",
                "# Working copy\n",
                "df = df_raw.copy()\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Exploring the Data (EDA)\n",
                "\n",
                "üèÉ **RUN** these cells to get an overview of the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df_info",
            "metadata": {},
            "outputs": [],
            "source": [
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df_describe",
            "metadata": {},
            "outputs": [],
            "source": [
                "df.describe(include='all')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda_questions",
            "metadata": {},
            "source": [
                "### ‚úèÔ∏è TODO: Answer the following questions (based on the outputs above)\n",
                "\n",
                "**Q1: How many features (columns) are in the dataset?**\n",
                "\n",
                "Your answer: ___\n",
                "\n",
                "**Q2: Which feature appears to be the target (the species we want to predict)?**\n",
                "\n",
                "Your answer: ___\n",
                "\n",
                "**Q3: Are there any missing values? If so, in which columns?**\n",
                "\n",
                "Your answer: ___"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "viz_header",
            "metadata": {},
            "source": [
                "### Visualizing the Relationships\n",
                "\n",
                "üèÉ **RUN** the pairplot and heatmap below.\n",
                "Lecture slides recommend checking **histograms** (distributions), **skewness**, and **correlations**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz_pairplot",
            "metadata": {},
            "outputs": [],
            "source": [
                "sns.pairplot(df, hue=\"species_short\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz_heatmap",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "numeric_df = df.select_dtypes(include=[np.number])\n",
                "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
                "plt.title(\"Correlation Heatmap\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cleaning_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Data Cleaning\n",
                "\n",
                "### Investigating Inconsistencies\n",
                "\n",
                "üèÉ **RUN** the following cell to inspect the unique values in the `sex` column."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "check_sex",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Unique Sex values:\", df['sex'].unique())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cleaning_todo",
            "metadata": {},
            "source": [
                "### ‚úèÔ∏è TODO: Handling the Error\n",
                "\n",
                "You likely noticed a `.` in the gender column. This means \"unknown\" or \"data entry error\".\n",
                "\n",
                "**Task**: Replace the `.` with `np.nan` so Python treats it as a proper missing value."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "replace_dot",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Write your code here to replace '.' with np.nan\n",
                "# Hint: df['sex'] = df['sex'].replace(..., ...)\n",
                "\n",
                "df['sex'] = df['sex'].replace('.', np.nan)\n",
                "print(\"Fixed Sex values:\", df['sex'].unique())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "missing_header",
            "metadata": {},
            "source": [
                "### Handling Missing Values\n",
                "\n",
                "üìñ **READ**: \n",
                "We have missing values in `sex` and some numerical columns.\n",
                "- **Dropping**: Quick, but loses data.\n",
                "- **Imputing**: Smart (filling with mean/median/mode).\n",
                "\n",
                "For this *manual* section, we will **drop** the rows with NAs to keep things simple. Later, in the **Pipelines** section, we will impute them intelligently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "drop_na",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Rows before dropping: {len(df)}\")\n",
                "df = df.dropna().reset_index(drop=True)\n",
                "print(f\"Rows after dropping: {len(df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "manual_prep_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Manual Preprocessing (Understanding the Mechanics)\n",
                "\n",
                "Before we automate with pipelines, let's understand how to transform data manually.\n",
                "\n",
                "### 1. Feature Engineering\n",
                "üèÉ **RUN** below to create a new feature `culmen_ratio`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feature_eng",
            "metadata": {},
            "outputs": [],
            "source": [
                "df['culmen_ratio'] = df['culmen_length_mm'] / df['culmen_depth_mm']\n",
                "df[['culmen_length_mm', 'culmen_depth_mm', 'culmen_ratio']].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "encoding_header",
            "metadata": {},
            "source": [
                "### 2. Encoding Categorical Variables\n",
                "\n",
                "üìñ **READ**:\n",
                "Models need numbers, not strings. We have two main ways to convert them:\n",
                "\n",
                "1.  **Label Encoding**: `0, 1, 2`. Good for ordinal data (Low, Med, High) or binary (Male, Female).\n",
                "2.  **One-Hot Encoding**: Creates new binary columns (`Is_Island_A`, `Is_Island_B`). Good for nominal data (Island, Color) where `0 < 1 < 2` doesn't make sense.\n",
                "\n",
                "‚úèÔ∏è **TODO**: Analyze the code below and run it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "manual_encoding",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example 1: Label Encoding for 'sex' (maps to 0 and 1)\n",
                "le = LabelEncoder()\n",
                "df['sex_encoded'] = le.fit_transform(df['sex'])\n",
                "\n",
                "# Example 2: One-Hot Encoding for 'island'\n",
                "df = pd.get_dummies(df, columns=['island'], prefix='island', drop_first=False)\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "scaling_header",
            "metadata": {},
            "source": [
                "### 3. Scaling Numerical Data\n",
                "\n",
                "üìñ **READ**:\n",
                "Features with large values (like `body_mass_g`: 4000) can dominate features with small values (`culmen_ratio`: 0.3). We use `StandardScaler` to put them on the same scale (mean=0, std=1).\n",
                "\n",
                "üèÉ **RUN** the Scaling step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "manual_scaling",
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "numeric_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'culmen_ratio']\n",
                "\n",
                "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pipeline_intro",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. The Professional Way: Pipelines & ColumnTransformer\n",
                "\n",
                "üìñ **READ**:\n",
                "Doing all the steps above manually (filling NAs, encoding, scaling) is messy and prone to error (Data Leakage). \n",
                "\n",
                "In the real world, we use **Pipelines**.\n",
                "A Pipeline bundles the steps together. A `ColumnTransformer` allows us to apply different pipelines to different columns (e.g., SimpleImputer for numbers, OneHotEncoder for text).\n",
                "\n",
                "### Reloading the \"Dirty\" Data\n",
                "Let's start fresh with the raw data to see the pipeline magic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pipeline_setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reload Raw Data\n",
                "df_full = pd.read_csv(url)\n",
                "X = df_full.drop('species_short', axis=1)\n",
                "y = df_full['species_short']\n",
                "\n",
                "# Fix the '.' error (This is usually done before the pipeline as a data cleaning step)\n",
                "X['sex'] = X['sex'].replace('.', np.nan)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pipeline_build",
            "metadata": {},
            "source": [
                "### ‚úèÔ∏è TODO: Build the Transformers\n",
                "\n",
                "We need two pipelines:\n",
                "1.  **Numeric**: Impute Missing (Median) -> Scale (StandardScaler)\n",
                "2.  **Categorical**: Impute Missing (Most Frequent) -> One-Hot Encode\n",
                "\n",
                "üèÉ **RUN** the cell below to define them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "define_pipeline",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Select Columns\n",
                "numeric_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
                "categorical_features = ['island', 'sex']\n",
                "\n",
                "# 2. Create Numeric Pipeline\n",
                "numeric_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "# 3. Create Categorical Pipeline\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
                "])\n",
                "\n",
                "# 4. Combine in ColumnTransformer\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numeric_transformer, numeric_features),\n",
                "        ('cat', categorical_transformer, categorical_features)\n",
                "    ])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pipeline_run_header",
            "metadata": {},
            "source": [
                "### Running the Full Pipeline\n",
                "\n",
                "Now we can process the entire dataset in effectively one line of code. This `fit_transform` handles the NA imputation, scaling, and encoding all at once."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pipeline_execute",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Execute the pipeline\n",
                "X_processed = preprocessor.fit_transform(X)\n",
                "\n",
                "# Convert back to DataFrame for viewing (Optional step for visualization)\n",
                "cat_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
                "all_names = numeric_features + list(cat_names)\n",
                "df_final = pd.DataFrame(X_processed, columns=all_names)\n",
                "\n",
                "print(\"Shape of processed data:\", df_final.shape)\n",
                "df_final.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "split_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Splitting the Data\n",
                "\n",
                "üìñ **READ**:\n",
                "To accurately evaluate a model, we must split our data into **Training** and **Test** sets. We train on the training set and validate on the test set.\n",
                "\n",
                "üèÉ **RUN** below to split the processed data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_test_split_exec",
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"Training set size: {X_train.shape}\")\n",
                "print(f\"Test set size: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "final_summary",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Summary\n",
                "\n",
                "In this lab, you learned how to:\n",
                "\n",
                "| Section | Technique | sklearn Class |\n",
                "| :--- | :--- | :--- |\n",
                "| **1. Exploration** | `.info()`, `.describe()`, Heatmap | - |\n",
                "| **2. Visualization** | Pairplot, Boxplot, Correlation | - |\n",
                "| **3. Cleaning** | Handling inconsistencies (replacing '.') | - |\n",
                "| **Feature Engineering** | Creating derived features (`culmen_ratio`) | - |\n",
                "| **4. Encoding** | Label Encoding, One-Hot Encoding | `LabelEncoder`, `OneHotEncoder` |\n",
                "| **5. Scaling** | Standard scaling (z-score) | `StandardScaler` |\n",
                "| **6. Pipelines** | Chaining imputation, scaling, and encoding | `Pipeline`, `ColumnTransformer` |\n",
                "| **7. Splitting** | Creating Train/Test sets | `train_test_split` |\n",
                "\n",
                "**Next steps:** In upcoming labs, you'll use these preprocessing techniques as part of full ML workflows!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
